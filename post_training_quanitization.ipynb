{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c97655-d7fa-48f6-8ffc-ea01fa8c4856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.quantization as quant\n",
    "from torch.quantization.observer import MovingAverageMinMaxObserver\n",
    "from torch.quantization.fake_quantize import FakeQuantize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1baf0925-91f6-494d-8b7e-b37ed00768c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: torch.Size([50000, 784]), torch.Size([50000])\n",
      "Valid: torch.Size([10000, 784]), torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "# Download & load MNIST\n",
    "data_dir = Path(\"data/mnist\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "url = \"https://github.com/pytorch/tutorials/raw/main/_static/mnist.pkl.gz\"\n",
    "fpath = data_dir / \"mnist.pkl.gz\"\n",
    "if not fpath.exists():\n",
    "    print(\"Downloading MNIST dataset...\")\n",
    "    fpath.write_bytes(requests.get(url).content)\n",
    "with gzip.open(fpath, \"rb\") as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "# Convert to tensors\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "x_valid = torch.tensor(x_valid, dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_valid, dtype=torch.long)\n",
    "print(f\"Train: {x_train.shape}, {y_train.shape}\")\n",
    "print(f\"Valid: {x_valid.shape}, {y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f766d20e-bdf4-4107-ad26-468a4e87a3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches: train=782, valid=79\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data augmentation & DataLoader definitions\n",
    "normalize = transforms.Normalize((0.1307,), (0.3081,))\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(), normalize,\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(0, translate=(0.1,0.1)),\n",
    "])\n",
    "valid_transforms = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images, self.labels, self.transform = images, labels, transform\n",
    "    def __len__(self): return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx].view(28,28).numpy()\n",
    "        img = self.transform(img) if self.transform else torch.tensor(img, dtype=torch.float32).unsqueeze(0)\n",
    "        return img, self.labels[idx]\n",
    "\n",
    "batch_size = 64\n",
    "train_dl = DataLoader(MNISTDataset(x_train, y_train, transform=train_transforms), batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(MNISTDataset(x_valid, y_valid, transform=valid_transforms), batch_size=batch_size*2)\n",
    "print(f\"Batches: train={len(train_dl)}, valid={len(valid_dl)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "043ef253-9064-4ba9-8055-7854a459b0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Ensure reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "678812ff-5b02-4fb8-b26a-6fbcfc57f929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set FBGEMM backend\n",
    "torch.backends.quantized.engine = 'fbgemm'\n",
    "\n",
    "# Define a Quantized‐ready wrapper with stubs\n",
    "class QuantizedMnistCNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quant = quant.QuantStub()\n",
    "        self.dequant = quant.DeQuantStub()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 2, 1), nn.BatchNorm2d(32), nn.ReLU(), nn.Dropout2d(0.1),\n",
    "            nn.Conv2d(32, 32, 3, 2, 1), nn.BatchNorm2d(32), nn.ReLU(), nn.Dropout2d(0.1),\n",
    "            nn.Conv2d(32, 16, 3, 2, 1), nn.BatchNorm2d(16), nn.ReLU(),\n",
    "            nn.Conv2d(16, 10, 3, 1, 1), nn.BatchNorm2d(10), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.net(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "680f8108-d861-4baa-9bfb-366acb2511ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAT Epoch 1: loss=0.1331\n",
      "QAT Epoch 2: loss=0.1300\n",
      "QAT Epoch 3: loss=0.1251\n",
      "QAT Epoch 4: loss=0.1244\n",
      "QAT Epoch 5: loss=0.1232\n",
      "Quantized teacher accuracy: 0.9838\n"
     ]
    }
   ],
   "source": [
    "# Reload float32 model and fuse Conv-BN-ReLU blocks\n",
    "teacher = QuantizedMnistCNN2().to('cpu')\n",
    "float_ckpt = torch.load(\"mnist_cnn2_final.pth\", map_location='cpu')\n",
    "# load only the 'net.' keys\n",
    "teacher_state = {k: v for k, v in float_ckpt.items() if k.startswith(\"net.\")}\n",
    "teacher.load_state_dict(teacher_state, strict=False)\n",
    "teacher.eval()\n",
    "quant.fuse_modules(\n",
    "    teacher.net,\n",
    "    [\n",
    "      [\"0\",\"1\",\"2\"],\n",
    "      [\"4\",\"5\",\"6\"],\n",
    "      [\"8\",\"9\",\"10\"],\n",
    "      [\"11\",\"12\",\"13\"]\n",
    "    ],\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Build custom QAT config (explicit quant_min/quant_max)\n",
    "teacher.train()\n",
    "qat_qconfig = quant.QConfig(\n",
    "    activation=FakeQuantize.with_args(\n",
    "        observer=MovingAverageMinMaxObserver,\n",
    "        quant_min=0, quant_max=255,\n",
    "        dtype=torch.quint8, qscheme=torch.per_tensor_affine\n",
    "    ),\n",
    "    weight=FakeQuantize.with_args(\n",
    "        observer=MovingAverageMinMaxObserver,\n",
    "        quant_min=-128, quant_max=127,\n",
    "        dtype=torch.qint8, qscheme=torch.per_tensor_symmetric\n",
    "    )\n",
    ")\n",
    "teacher.qconfig = qat_qconfig\n",
    "quant.prepare_qat(teacher, inplace=True)\n",
    "\n",
    "# Fine-tune QAT for a few epochs\n",
    "optimizer = optim.SGD(teacher.parameters(), lr=1e-3, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "for epoch in range(5):\n",
    "    teacher.train()\n",
    "    running = 0.0\n",
    "    for xb, yb in train_dl:  # MNIST DataLoader\n",
    "        optimizer.zero_grad()\n",
    "        preds = teacher(xb)\n",
    "        loss = loss_fn(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running += loss.item() * xb.size(0)\n",
    "    print(f\"QAT Epoch {epoch+1}: loss={running/len(train_dl.dataset):.4f}\")\n",
    "\n",
    "# 5) Convert to fully quantized INT8\n",
    "teacher.eval()\n",
    "quantized_model = quant.convert(teacher, inplace=False)\n",
    "\n",
    "# 6) Script & save\n",
    "scripted = torch.jit.script(quantized_model)\n",
    "scripted.save(\"mnist_cnn2_int8_scripted.pt\")\n",
    "\n",
    "# 7) Sanity-check directly with FloatTensor input\n",
    "x_val = ((x_valid - 0.1307)/0.3081).view(-1,1,28,28)\n",
    "with torch.no_grad():\n",
    "    logits = scripted(x_val)             \n",
    "    preds  = logits.argmax(dim=1).numpy()\n",
    "print(\"Quantized teacher accuracy:\", accuracy_score(y_valid.numpy(), preds).round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3ea077-6aad-4f8a-bc87-1a60e2d06ebb",
   "metadata": {},
   "source": [
    "**Prediction Instance on The Quantized Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b44b067a-e3c7-49ee-9492-6080d416571f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Test Accuracy: 98.38%\n",
      "122\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAEmCAYAAABLZ43dAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAewElEQVR4nO3deVxU9f4/8NewDwMKCAi4IHjBLdQkypXcxeWhKbinmFjei1etbpp+TQXLfNzcEART73W5SpjZdi2zBSsXrIt1vUbqjYsl2aLglgomy/v3B485P8eBmWET9PN6Ph7zB+fzmXPec5jXnDNn+YxORAREdF+za+gCiKj+MehECmDQiRTAoBMpgEEnUgCDTqQABp1IAQw6kQIYdCIFMOg2aNOmDaZNm6b9/dlnn0Gn0+Gzzz5rsJrudGeNRLdr9EHftm0bdDqd9nBxcUFoaCj+/Oc/4/z58w1dXrXs27cPCQkJDV1GlfLy8jBp0iT4+vpCr9cjJCQEixYtqpN5T5s2zeT/WNWjsX9Yvf766+jRowcMBgM8PDzQs2dPHDhwoKHLssqhoQuw1bJlyxAUFISbN2/i8OHD2LBhA/bt24ecnBy4urre1VoiIyNRXFwMJyenaj1v3759SE1NbZRhP378OPr27YsWLVrgL3/5C5o1a4b8/Hz8+OOPdTL/mTNnYuDAgdrf33//PZYsWYKnnnoKffr00aa3bdu2TpZXHxISErBs2TLExMRg2rRpKCkpQU5ODn766aeGLs06aeS2bt0qACQ7O9tk+rPPPisA5LXXXqvyudevX6+TGgIDAyU2NrbW85k1a5bU1yqvTY1lZWXywAMPyCOPPCJFRUV1W1gVsrOzBYBs3brVYr+6+h/W1tGjR0Wn08maNWsaupQaafS77lXp378/gIotA1Cxa+jm5oa8vDwMGzYM7u7umDx5MgCgvLwcSUlJ6NSpE1xcXNC8eXPMnDkTly9fNpmniOCll15Cy5Yt4erqin79+uHbb781W3ZV39G//PJLDBs2DJ6enjAYDOjcuTPWrVun1ZeamgoAJruqRnVdI1CxK56Xl2d1XX700UfIycnB0qVLodfrUVRUhLKyMqvPq2vGr2mff/454uPj4evri5YtWwKoWH9t2rQxe05CQoLJejTauXMnwsPDodfr4eXlhQkTJpjtnRQVFeH06dMoLCy0WltSUhL8/Pwwd+5ciAiuX79esxfZQO7ZoBvfwM2aNdOmlZaWYsiQIfD19cWqVasQHR0NoGK3cd68eejVqxfWrVuHJ554Aunp6RgyZAhKSkq05y9ZsgSLFy9Gly5dsHLlSgQHB2Pw4MG4ceOG1Xo+/vhjREZG4uTJk5g7dy5Wr16Nfv364b333tNqGDRoEABgx44d2sOoPmocMGAABgwYYLX2Tz75BADg7OyMhx56CAaDAa6urpgwYQIuXbpk9fl1LT4+HidPnsSSJUuwYMGCaj9/+fLlmDp1KkJCQrBmzRo8/fTTyMzMRGRkJK5cuaL1+9e//oUOHTpg/fr1VueZmZmJiIgIJCcnw8fHB+7u7vD397fpuY1CA+9RWGXcdf/kk0+koKBAfvzxR9m1a5c0a9ZM9Hq9nDt3TkREYmNjBYAsWLDA5PmHDh0SAJKenm4yff/+/SbTL1y4IE5OTjJ8+HApLy/X+v3f//2fADDZLf70008FgHz66aciIlJaWipBQUESGBgoly9fNlnO7fOqate9PmoUqdidDwwMNFvenUaOHCkApFmzZjJ58mTZs2ePLF68WBwcHKRnz54my6orle26G//XvXv3ltLSUpP+sbGxlb6WpUuXmqzTH374Qezt7WX58uUm/b755htxcHAwmW78Py5dutRirZcuXdLWj5ubm6xcuVJef/11iYqKEgDy6quv2v7CG8g9E/Q7H4GBgbJ//36tnzHoZ8+eNXn+nDlzpGnTpnLhwgUpKCgwebi5ucmMGTNEROS1114TACbzFKkIl7WgG9+0a9eutfhaqgp6fdRYHf379xcAEhUVZTJ9xYoVAkA+/vjjGs3XEktB3759u1l/W4O+Zs0a0el0kpuba7YuO3ToIAMHDqx2rfn5+dr7bteuXdr0srIy6dixo7Rs2bLa87zb7pmj7qmpqQgNDYWDgwOaN2+Odu3awc7O9JuHg4OD9p3OKDc3F1evXoWvr2+l871w4QIA4OzZswCAkJAQk3YfHx94enparM34NeKBBx6w/QXd5Rot0ev1AICJEyeaTJ80aRIWLlyIrKwskyPm9S0oKKjGz83NzYWImK0jI0dHx2rP07h+HB0dERMTo023s7PD+PHjsXTpUuTn56N169Y1K/ouuGeC/vDDD+Ohhx6y2MfZ2dks/OXl5fD19UV6enqlz/Hx8amzGmuqoWsMCAgAADRv3txkuvGD584DgvXNGKzbVXbADYDZQcPy8nLodDp88MEHsLe3N+vv5uZW7Xq8vLzg4uICDw8Ps3nevo4Y9AbUtm1bfPLJJ+jVq1elbyCjwMBAABVbhODgYG16QUGB1Te68dxvTk6OxS1fVW/Wu1GjJeHh4di8ebPZ+eCff/4ZQOP4MPT09DQ5kGZk3Msxatu2LUQEQUFBCA0NrZNl29nZoWvXrsjOzsatW7dMrp9oTOvIknv2qLutxo0bh7KyMrz44otmbaWlpdqbZ+DAgXB0dERKSgrktvEyk5KSrC6jW7duCAoKQlJSktmb8fZ5GQwGADDrU1812np6bdSoUXB2dsbWrVtRXl6uTf/b3/4GANrZgobUtm1bXL16FSdOnNCm/fLLL3j77bdN+o0ZMwb29vZITEw0WUdAxf/i4sWL2t/VOb02fvx4lJWVYfv27dq0mzdvIj09HR07dtT2ihqtBjw+YJOqLpi5U2xsrBgMhkrbZs6cKQBk6NChsnbtWlm/fr3MnTtXAgIC5I033tD6LVy4UADIsGHDZP369RIXFycBAQHi7e1t8WCcSMURckdHRwkMDJSEhATZuHGjPPPMMzJ48GCtz+7duwWATJkyRXbu3CkZGRn1VqOI7UfdRUSWLVsmAGTQoEGSmpoqTz31lOh0Opk4caJNz68uSwfjKvtfFxYWisFgkODgYElKSpKXX35ZWrVqJd26dTM7wGk8iNizZ0955ZVXZMOGDTJ//nwJCQmRlStXav1sPeouIlJUVCSdOnUSR0dHee655yQ5OVkiIiLE3t5e9u3bV+P1cLcoEXQRkU2bNkl4eLjo9Xpxd3eXsLAwmT9/vvz8889an7KyMklMTBR/f3/R6/XSt29fycnJMbvqrLKgi4gcPnxYBg0aJO7u7mIwGKRz586SkpKitZeWlsrs2bPFx8dHdDqd2Ru0LmsUqV7Qy8vLJSUlRUJDQ8XR0VFatWolL7zwgty6dcum51dXdYMuIvLRRx/JAw88IE5OTtKuXTvZuXOn2VF3ozfffFN69+4tBoNBDAaDtG/fXmbNmiX//e9/tT7VCbqIyPnz5yU2Nla8vLzE2dlZHnnkEbMzII2VToTjuhPd7+777+hExKATKYFBJ1IAg06kAAadSAEMOpECGHQiBTR40G0ZMLCuRlwtKipCQkJCjea1b98+6HQ6BAQEmFwmSjWza9cudOvWDS4uLvDx8UFcXFyll6JevXoV8+fPR0hICPR6PQIDAxEXF4f8/Hyry/j2228xduxYBAcHw9XVFd7e3oiMjMTevXsr7b979250794dHh4eaNasGR599FG8//77Jn2uXLmCyZMnw9PTE8HBwfj73/9uNp9jx47B1dVVG/2oUWjoK3Z27Nhh8hg0aJAAMJv+66+/1npZBQUF1boS6naTJk2SNm3a1Nv92SpJS0sTADJgwABJTU2VhQsXiqurq3Tu3FmKi4u1fmVlZRIRESEGg0HmzZsnmzdvlueff17c3d2lRYsW8ttvv1lczvvvvy9DhgyRhIQE2bRpkyQlJUmfPn0EgGzcuNGkb3JysgCQ4cOHy4YNG2Tt2rXSpUsXASBvvvmm1s94yfG6detk9uzZotPp5MiRI1p7eXm59OjRQxYuXFhHa6tuNHjQ71SfAyjWNOjXr18Xg8EgycnJ8uCDD8q0adPqpb660FgGU6zK77//Lh4eHhIZGWkycs3evXsFgCQnJ2vTjhw5IgBk/fr1JvPYsmWLAJC33nqr2ssvLS2VLl26SLt27Uymh4SESEREhElNV69eFTc3Nxk5cqQ2rXnz5iYDYzz66KMmoxrt2LFDAgIC5Nq1a9WurT41+K67LWwdOPHYsWMYMmQIvL29odfrERQUhOnTpwMAfvjhB+1WwsTERO0rgS1DL7/99tsoLi7G2LFjMWHCBLz11lu4efOmWb+bN28iISEBoaGhcHFxgb+/P8aMGWNyB1l5eTnWrVuHsLAwbbc1KioKx44d0+rU6XTYtm2b2fzvrNc4MOLJkycxadIkeHp6onfv3gCAEydOYNq0aQgODoaLiwv8/Pwwffp0k7u3jH766SfExcUhICAAzs7OCAoKwp/+9CfcunULZ86cgU6nw9q1a82el5WVBZ1Oh4yMDJvvBMvJycGVK1cwfvx4k9t2R4wYATc3N+zatUub9ttvvwEwv0/e398fQOX3rVtjb2+PVq1amd1B+Ntvv8HX19ekpiZNmsDNzc1kOcXFxSaDfHh5eaGoqAgAcOPGDSxYsAArVqyo0X3v9aqhP2nuVNkWfcaMGeLg4CBPPvmkvPrqq/L888+LwWCQiIgI7aaL8+fPi6enp4SGhsrKlStl8+bNsmjRIunQoYOIVGzpNmzYIABk9OjR2leC//znP1ZrioqKkgEDBoiIyNmzZ0Wn08nu3btN+pSWlsqAAQMEgEyYMEHWr18vK1askP79+8s777yj9Zs2bZp2l1pSUpKsWrVKRo0apd388v3331c5DDLu2Bsx3tDRsWNHGTVqlKSlpUlqaqqIiKxatUr69Okjy5Ytk02bNsncuXNFr9fLww8/bLLV+umnnyQgIEBcXV3l6aeflldffVUWL14sHTp00Ma/69Wrl4SHh5vVEx8fL+7u7nLjxg2bbxDJysoSALJlyxazNh8fH9Hr9VJWViYiFXtgxhtSMjMz5dy5c/LZZ59JWFiYRERESElJicVlGV2/fl0KCgrkf//7n6xZs0bs7e1l0qRJJn3Gjx8v9vb2kpycLN9//72cOnVK4uPjRa/XS1ZWltZvwIAB0rdvX/nuu+9k//79otfrZefOnSJSMXbfneu3sWj0Qbd14MS3337b6l1uNdl1P3/+vDg4OMjmzZu1aT179pRRo0aZ9DPuTlY27rfxH3/gwAEBIHPmzKmyT02CXtmtpJWNz56RkSEA5ODBg9q0qVOnip2dXaXrzVjTxo0bBYCcOnVKa7t165bJrbG2Br2goEB0Op3ExcWZTD99+rQ2LlthYaE2/b333hN/f3+T8QKHDBlSrV1j4y3AAMTOzk5iYmLk0qVLJn3Onz+vfVAbH97e3iYhFxE5ceKEtGzZUusTHR0tZWVlcubMGdHr9XL06FGb67qbGn3QbR048fY3WlW3VtYk6OvWrRMnJyeTN0ZKSorZtOHDh4u3t7fFrcysWbNEp9PJxYsXq+xTk6B//vnnFl9DcXGxFBQUaPNOSkoSkYqDXU2aNDH70LrT5cuXxcXFRV544QVtmvE7dU0OTI4fP14cHBxk1apVkpeXJwcPHpQuXbqIo6OjAJAff/xR6/vll1/KsGHDZPny5fLOO+9IQkKCuLq6SkxMjM3LO3XqlHz88ceyfft2GT58uIwePdrs4O61a9ckPj5eYmNj5Y033pAtW7ZIWFiY+Pn5SW5urknf4uJiyc7ONpk+evRoefzxx0Wk4hbZzp07S5s2bSQxMbFRbOEbfdCHDh1a6SiwxofxQEl5eblER0cLAGnSpImMHDlStmzZIjdv3tTmVZOgR0RESO/evSU3N1d7HD582OzIbfv27aVXr14W5xUVFSUtWrSw2KcmQc/Pzzfre/HiRZkzZ474+vqarbPExEQREfn1118FgCxatMhiTSIiY8eOleDgYO3vCRMmSIsWLbTd7Oq4cuWKNsS08fH444/LmDFjBID2lSEvL09cXV1lz549Js/ftm2bAKjxgA+DBg0yO/AWFRUlI0aMMOl38eJF8fLyknHjxlmcX2ZmphgMBjl37pycPn1aHB0dZcuWLXLgwAFp3rx5pV9T7rZGP2acrQMn6nQ67NmzB1988QX27t2LDz/8ENOnT8fq1avxxRdf1OjgSG5uLrKzswGYj7wKAOnp6XjqqaeqPV9LbB0E8XaVHZQaN24csrKyMG/ePHTt2hVubm4oLy9HVFRUja4DmDp1Kt544w1kZWUhLCwM//znPxEfH282GKctmjZtinfffRf5+fn44YcfEBgYiMDAQPTs2RM+Pj7w8PAAUPHLLTdv3sSIESNMnj9y5EgAwJEjRzB06NBqLz8mJgYzZ87Ed999h3bt2uHMmTPYv38/Nm3aZNLPy8sLvXv3xpEjR6qcV1lZGebOnYsFCxagRYsWePHFF9GzZ0888cQTACp+mCM9PV37u6E0+qDbOnCiUffu3dG9e3csX74cr732GiZPnoxdu3ZhxowZVYaoKunp6XB0dMSOHTvMRv88fPgwkpOTtWF+27Ztiy+//BIlJSVVDinctm1bfPjhh7h06RK8vLwq7WM8onvnUeE7B0G05PLly8jMzERiYiKWLFmiTc/NzTXp5+PjgyZNmiAnJ8fqPKOiouDj44P09HQ88sgjKCoqwpQpU2yuqTKtW7fWRk69cuUKvvrqK+3XdQDg/PnzEBGzDznjL9eUlpbWaLnFxcUAKi7GMS4HqPzDtKSkxOJyNmzYgGvXruG5554DUDFY5O3jxwUEBDSKH2Fs9KfXbB048fLly2aDAXbt2hUA8PvvvwOA9qurlY0mWpn09HT06dMH48ePR0xMjMlj3rx5AICMjAwAQHR0NAoLCyv9iR5jXdHR0RARJCYmVtmnSZMm8Pb2xsGDB03a09LSbKoZgPahdOf6uHMQSTs7Ozz22GPYu3evdnqvspqAijHzJ06ciN27d2Pbtm0ICwtD586dtfbqDLRYmYULF6K0tBTPPPOMNi00NBQigt27d5v0Na7zBx98UJtWWFiI06dPa6e6gP8/Hv7tSkpK8I9//AN6vR4dO3YEAPzhD3+AnZ0dXn/9dZPXfO7cORw6dMhkObe7dOkSli5dipUrV8LFxQVAxanA06dPa31OnToFPz8/m9dDvWmwLw1VqOz0mi0DJ65du1ZCQkJk/vz5snHjRlm1apW0a9dOmjRpImfOnNHm1bFjR/Hz85PU1FTJyMiQb775ptI6vvjiC5MDV5UJDw+XsLAwEak4vda3b1/t9Fpqaqq88sorMnjwYJPTa1OmTNFey7p162Tt2rUyZswYk7HlFixYIAAkLi5ONmzYIBMnTpTw8PAqv6MXFBSY1RYZGSmurq6yaNEiSUtLk8cee0y70uv2eZw7d078/Py002sbN26UhIQE6dSpk9nPSx07dkz7Tv3Xv/7VpK0646+tWLFCJk+eLMnJyZKWliaDBw8WAPLSSy+Z9CssLBQ/Pz9xcnKSOXPmyMaNG2XmzJlib28vnTp1kt9//91sXdw+jt9jjz0m/fv3l4SEBNm8ebO8+OKL0r59ewEgq1evNlnWjBkzBID069dPUlJS5OWXX5aWLVuKvb19lQc74+Pj5dFHHzWZduLECdHpdPLHP/5RVqxYIS4uLpKWlmZ1ndS3eyLoItYHTvz6669l4sSJ0rp1a3F2dhZfX18ZMWKEHDt2zGQ+WVlZEh4eLk5OThbfmLNnzxYAkpeXV2WtCQkJAkA7F19UVCSLFi2SoKAgcXR0FD8/P4mJiTGZR2lpqaxcuVLat28vTk5O4uPjI0OHDpWvvvpK61NUVCRxcXHStGlTcXd3l3Hjxmk/u2Rr0M+dOyejR48WDw8Padq0qYwdO1Z+/vnnSl/z2bNnZerUqeLj4yPOzs4SHBwss2bNMgmSUadOncTOzk77zTuj6gT9vffek4cffljc3d3F1dVVunfvbnZdwu2vY/r06RIUFCROTk7i7+8vTz75pNlrrizoGRkZMnDgQGnevLk4ODiIp6enDBw4UN59912z5ZSUlEhKSop07dpV3NzcxM3NTfr16ycHDhyotK4TJ06Ik5OT/Pvf/zZr27Ztm7Rp00aaNWsmzz77rNnvyDUEDg5J1fLggw/Cy8sLmZmZDV0KVUOj/45OjcexY8dw/PhxTJ06taFLoWriFp2sysnJwVdffYXVq1ejsLAQZ86c0Q4+0b2BW3Syas+ePXjiiSdQUlKCjIwMhvwexC06kQK4RSdSAINOpAAGnUgBNl/rXt3rxIno7rDlMBu36EQKYNCJFMCgEymAQSdSAINOpAAGnUgBDDqRAhh0IgUw6EQKYNCJFMCgEymAQSdSAINOpAAGnUgBDDqRAhr9b69R3bPlN+yef/55i+0REREW22NiYiy2G3//jO4ObtGJFMCgEymAQSdSAINOpAAGnUgBDDqRAhh0IgUw6EQK4AUz9yF3d3eL7du3b7c6j5EjR1pst/aDHtHR0Rbbd+7cabUGqjvcohMpgEEnUgCDTqQABp1IAQw6kQIYdCIFMOhECuB59PtQjx49LLZbO0dui+PHj1tsz8zMrPUyqO5wi06kAAadSAEMOpECGHQiBTDoRApg0IkUwKATKYDn0e9Bnp6eFtvnz59f7zW88MILFtt/+eWXeq+BbMctOpECGHQiBTDoRApg0IkUwKATKYBBJ1IAg06kAJ2IiE0drYzjTXXHYDBYbN+3b5/F9l69etW6hs6dO1tsP3nyZK2XQXXDlghzi06kAAadSAEMOpECGHQiBTDoRApg0IkUwKATKYBBJ1IAB55ohMaNG2exvXfv3rWa/6FDh6z2OXv2bK2WQY0Lt+hECmDQiRTAoBMpgEEnUgCDTqQABp1IAQw6kQI48EQjVFZWZrHd2r8sJyfHYnu/fv2s1nD58mWrfahx4MATRASAQSdSAoNOpAAGnUgBDDqRAhh0IgUw6EQK4P3od1mbNm3qfRlHjx612M5z5OrhFp1IAQw6kQIYdCIFMOhECmDQiRTAoBMpgEEnUgDPo99lixcvrvU8vvvuO4vt8+fPr/Uy6P7CLTqRAhh0IgUw6EQKYNCJFMCgEymAQSdSAINOpAAGnUgBvGCmjnXq1Mlie3R0dK2XcejQIYvt165dq/Uy6P7CLTqRAhh0IgUw6EQKYNCJFMCgEymAQSdSAINOpACeR69j3bp1s9ju7u5udR52dpY/f62dRye6E7foRApg0IkUwKATKYBBJ1IAg06kAAadSAEMOpECeB79LhMRq33Ky8vvQiWWWbuvvkePHhbbs7Oza12DtR+qKC4urvUyVMEtOpECGHQiBTDoRApg0IkUwKATKYBBJ1IAg06kAJ5Hvw9ZGzv+ySeftDqPyMhIi+3Ozs4W2225XsCao0ePWmyfPXu2xfbjx4/Xuob7BbfoRApg0IkUwKATKYBBJ1IAg06kAAadSAEMOpECGHQiBejExisbdDpdfddyX5gyZYrF9q1bt1qdh7V1nZ+fb7Hd09PTYrubm5vVGqyxVmNdXDBjzQcffGCxfcGCBRbbv/3227osp8HYsq65RSdSAINOpAAGnUgBDDqRAhh0IgUw6EQKYNCJFMCBJ+5BrVq1qtXzT548abXPyJEjLbZb+/EEb29vi+1Tp061WkNsbKzF9qFDh1psv3HjhsX2CRMmWK3hfsEtOpECGHQiBTDoRApg0IkUwKATKYBBJ1IAg06kAN6Pfpfl5eVZ7RMcHGyxvby8vFY1DBw40GqfgwcPWmwvKyurVQ222Lx5s8X26dOnW2y/cOGCxXZ/f/9q19QY8X50IgLAoBMpgUEnUgCDTqQABp1IAQw6kQIYdCIF8Dz6XWbtHmsA2LJli8X2uzFm+sKFCy22f/TRR7Waf2BgoNU+1taDh4eHxXZr59EDAgKs1nAv4Hl0IgLAoBMpgUEnUgCDTqQABp1IAQw6kQIYdCIFMOhECuAFM42QtUEd7sYFM9ZYez/cjRrPnj1rsT0tLc1i++rVq+uynAbDC2aICACDTqQEBp1IAQw6kQIYdCIFMOhECmDQiRTg0NAFkLlevXpZbN+xY4fF9qCgoLosp8G88sorFttTUlIstv/yyy91Wc49jVt0IgUw6EQKYNCJFMCgEymAQSdSAINOpAAGnUgBvB/9HtS0aVOL7Y8//rjF9tatW9e6hoiICIvt2dnZFtvffPNNq8v4+uuvLbaXlpZanYcKeD86EQFg0ImUwKATKYBBJ1IAg06kAAadSAEMOpECeB6d6B7H8+hEBIBBJ1ICg06kAAadSAEMOpECGHQiBTDoRApg0IkUwKATKYBBJ1IAg06kAAadSAEMOpECGHQiBTDoRApg0IkUwKATKYBBJ1IAg06kAAadSAEMOpECGHQiBTDoRApg0IkUwKATKYBBJ1IAg06kAAadSAEMOpECGHQiBTDoRApg0IkUwKATKYBBJ1IAg06kAAadSAEMOpECGHQiBTDoRApg0IkUwKATKYBBJ1IAg06kAAadSAEMOpECGHQiBTDoRApg0IkUwKATKcDB1o4iUp91EFE94hadSAEMOpECGHQiBTDoRApg0IkUwKATKYBBJ1IAg06kAAadSAH/D/FXis2/CWi0AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# using the same quant backend\n",
    "torch.backends.quantized.engine = 'fbgemm'\n",
    "\n",
    "# 1) Load the INT8 model\n",
    "model = torch.jit.load(\"mnist_int8.pt\", map_location=device)\n",
    "model.eval()\n",
    "\n",
    "# 2) Preprocess the validation set exactly as in training\n",
    "mean, std = 0.1307, 0.3081\n",
    "x_norm = (x_valid - mean) / std\n",
    "x_norm = x_norm.view(-1, 1, 28, 28)  # → [10000,1,28,28]\n",
    "\n",
    "# 3) Run a full‐batch inference\n",
    "with torch.no_grad():\n",
    "    logits = model(x_norm.to(device))         \n",
    "    preds  = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "# 4) Compute overall accuracy\n",
    "y_true      = y_valid.cpu().numpy()\n",
    "overall_acc = accuracy_score(y_true, preds)\n",
    "print(f\"Overall Test Accuracy: {overall_acc:.2%}\")\n",
    "\n",
    "# 5) Visualize a single example\n",
    "i = 122  \n",
    "print(i)\n",
    "img    = x_norm[i].squeeze().cpu().numpy()   # [28,28]\n",
    "pred_i = preds[i]\n",
    "true_i = int(y_valid[i].item())\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(\n",
    "    f\"Predicted: {pred_i}   True: {true_i}\\n\"\n",
    "    f\"Test Accuracy: {overall_acc:.2%}\",\n",
    "    fontsize=12\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e85cb3a-9e8d-48ab-aa06-81f2844aea43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
